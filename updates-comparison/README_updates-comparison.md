Updates Comparison â€“ Expected vs. Sample Updates
Overview

This project compares expected updates and sample updates in value estimation, analyzing how computation cost vs. accuracy scales with the branching factor (b).
It addresses the key question:

â€œIs it better to perform one expensive expected update or many cheaper sample updates?â€

Where Itâ€™s Used

Used in reinforcement learning to understand efficiency trade-offs in planning and value function estimation â€” foundational to algorithms like Q-learning and SARSA.

Files

expectation_vs_sample.py â€“ Implements calculate_errors(branching_factor) to compute RMSE for various branching factors.

expectation_vs_sample.ipynb â€“ Simulates expected vs. sample updates for b âˆˆ {2, 10, 100, 1000} and visualizes convergence.

requirements.txt â€“ Dependencies.

README.md â€“ Documentation.

How It Works

Expected update:

ğ‘„
(
ğ‘ 
,
ğ‘
)
â†
âˆ‘
ğ‘ 
â€²
ğ‘
(
ğ‘ 
â€²
âˆ£
ğ‘ 
,
ğ‘
)
[
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
â€²
)
]
Q(s,a)â†
s
â€²
âˆ‘
	â€‹

p(s
â€²
âˆ£s,a)[r+Î³
a
â€²
max
	â€‹

Q(s
â€²
,a
â€²
)]

Accurate but requires evaluating b next states.

Sample update:

ğ‘„
(
ğ‘ 
,
ğ‘
)
â†
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
â€²
)
Q(s,a)â†r+Î³
a
â€²
max
	â€‹

Q(s
â€²
,a
â€²
)

Cheaper â€” updates using one sampled transition, introducing noise but scaling better for large b.

Simulation Steps:

Generate random next-state values (âˆ¼ N(0, 1))

Compute true expected value

Iteratively sample and update

Repeat 100 runs and average RMSE

Parameters
branching_factors = [2, 10, 100, 1000]
runs = 100
samples_per_run = 2 * branching_factor

Results

X-axis: Computation (normalized by b)

Y-axis: RMS error in value estimates

Findings:

Small b â†’ expected updates more accurate

Large b â†’ sample updates achieve similar accuracy faster

Sampling scales better â€” ideal for large environments

Summary

Expected updates = accurate but costly
Sample updates = noisy but efficient
â†’ In large-scale RL, sample-based methods dominate, forming the foundation of modern learning algorithms.