# ðŸš— Mountain Car â€“ Semi-Gradient *n-Step SARSA* with Tile Coding

This project implements **Semi-Gradient n-Step SARSA** with **tile coding** on the classic **Mountain Car** reinforcement learning problem.
It reproduces and extends the experiments from *Sutton & Barto (2018), â€œReinforcement Learning: An Introduction,â€* Chapter 10 â€” Figures 10.1 â€“ 10.4.

---

## ðŸ§  Background

The **Mountain Car** task is a continuous-state control problem:

* **Goal:** drive the car to the top of the hill (position â‰¥ 0.5).
* **State:**

  * Position âˆˆ [ âˆ’1.2 , 0.5 ]
  * Velocity âˆˆ [ âˆ’0.07 , 0.07 ]
* **Actions:**

  * âˆ’1 â†’ push left
  * 0  â†’ no push
  * +1 â†’ push right
* **Reward:** âˆ’1 at each step until the goal is reached.

Because the carâ€™s engine is too weak to climb directly, it must first back up to gain momentum â€” making it an ideal benchmark for temporal-difference control algorithms with function approximation.

---

## âš™ï¸ Algorithm Overview

### ðŸ”¹ Semi-Gradient *n-Step SARSA*

An on-policy TD control algorithm that updates the action-value function Q(s, a) using *n-step returns*.

**Update target:**

[
G_t^{(n)} = R_{t+1} + R_{t+2} + \dots + R_{t+n} + Q(S_{t+n}, A_{t+n})
]

**Weight update:**

[
\mathbf{w} \leftarrow \mathbf{w} + \alpha \big( G_t^{(n)} - \hat{Q}(S_t, A_t; \mathbf{w}) \big) \nabla_\mathbf{w} \hat{Q}(S_t, A_t; \mathbf{w})
]

When *n = 1*, this reduces to standard one-step SARSA.

---

### ðŸ”¹ Tile Coding (Function Approximation)

Tile coding discretizes continuous variables into overlapping *tilings*.

Each tiling produces one active feature for a given (state, action), and the estimated value is the sum of the corresponding weights.

Advantages:

* Efficient linear approximation
* Generalization across nearby states
* Fast and sparse updates

---

## ðŸ§© Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mountain_car.py     # Environment, n-step SARSA, plotting utilities
â”‚   â””â”€â”€ tile_coding.py      # Sutton's tile coding implementation (IHT, tiles)
â”œâ”€â”€ book_images/            # Reference figures from Sutton & Barto
â”œâ”€â”€ generated_images/       # Output of trained runs (Figures 10.1â€“10.4)
â””â”€â”€ README.md
```

---

## ðŸ“¦ Requirements

**Python â‰¥ 3.8**

Install dependencies:

```bash
pip install numpy matplotlib tqdm ipython
```

Optional (for interactive work):

```bash
pip install jupyterlab
```

---

## ðŸš€ How to Run

### 1ï¸âƒ£ Train and Visualize Learning

```bash
python src/mountain_car.py
```

This script will:

* Run 9 000 episodes of training.
* Plot the **cost-to-go** surface at selected episodes (Figure 10.1).
* Compare learning curves for different step-sizes (Figure 10.2).
* Compare 1-step vs 8-step SARSA (Figure 10.3).
* Explore the combined effect of Î± and n (Figure 10.4).

Results are saved automatically in:

```
/generated_images/
```

---

## ðŸ“Š Output Figures

|      Figure     | Description                                   |
| :-------------: | :-------------------------------------------- |
| **Figure 10.1** | Evolution of cost-to-go surface over training |
| **Figure 10.2** | Learning curves for various step sizes Î±      |
| **Figure 10.3** | Comparison of n = 1 vs n = 8 SARSA            |
| **Figure 10.4** | Combined effect of Î± and n on learning speed  |

Each corresponds directly to examples from Sutton & Barto (2018).

---

## ðŸ§© Key Classes and Functions

| Name                                             | Purpose                                      |
| ------------------------------------------------ | -------------------------------------------- |
| `ValueFunction`                                  | Linear value approximator using tile coding  |
| `get_action(position, velocity, value_function)` | Îµ-greedy policy for action selection         |
| `step(position, velocity, action)`               | Physics simulation of the car                |
| `semi_gradient_n_step_sarsa(value_function, n)`  | Core training loop implementing n-step SARSA |
| `print_cost(value_function, episode, ax)`        | 3D visualization of cost-to-go surface       |

---

## ðŸ“ˆ Core Parameters

| Parameter        | Description                      | Typical Value        |
| ---------------- | -------------------------------- | -------------------- |
| `num_of_tilings` | Number of overlapping tilings    | 8                    |
| `step_size (Î±)`  | Learning rate divided by tilings | 0.3 / num_of_tilings |
| `n_steps`        | Number of bootstrapping steps    | 1 or 8               |
| `episodes`       | Training iterations              | 9 000 (max)          |

---

## ðŸ” Research Context and Use Cases

This project demonstrates:

* Function approximation with **tile coding**
* **n-step bootstrapping** vs 1-step methods
* The impact of **Î± (step-size)** and **n (update horizon)**
* Continuous-state control with **TD learning**

Itâ€™s valuable for:

* Reproducing textbook results for study or teaching
* Experimenting with multi-step TD methods
* Serving as a baseline for extensions such as:

  * Eligibility traces (Î»)
  * Off-policy variants (e.g. Q(Ïƒ))
  * Actorâ€“Critic algorithms

---

## ðŸ“š References

1. Sutton, R. S., & Barto, A. G. (2018).
   *Reinforcement Learning: An Introduction (2nd Edition).* MIT Press.
   [http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)

2. Mountain Car environment (Example 10.1 â€“ 10.4).



> *â€œGeneralization is the crux of learning â€” tile coding makes it possible for continuous worlds.â€*
> â€” Sutton & Barto (2018)
