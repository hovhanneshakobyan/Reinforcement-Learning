Random Walk with Function Approximation
Overview

This project explores n-step Temporal Difference (TD) learning with function approximation, extending the classical Random Walk task from reinforcement learning.
It investigates how different feature representations â€” polynomial, Fourier, state aggregation, and tile coding â€” affect the stability, convergence, and accuracy of value estimation.

This concept is fundamental in reinforcement learning, prediction problems, and function approximation in continuous or large discrete spaces.

How It Works
Environment

Linear chain of states (e.g., 19 states)

Start from the center; move left or right with equal probability

Rewards:

âˆ’1 at left terminal

+1 at right terminal

0 otherwise

n-step TD Learning

The update uses the n-step return:

ğº
ğ‘¡
(
ğ‘›
)
=
âˆ‘
ğ‘˜
=
0
ğ‘›
âˆ’
1
ğ›¾
ğ‘˜
ğ‘…
ğ‘¡
+
ğ‘˜
+
1
+
ğ›¾
ğ‘›
ğ‘‰
(
ğ‘†
ğ‘¡
+
ğ‘›
)
G
t
(n)
	â€‹

=
k=0
âˆ‘
nâˆ’1
	â€‹

Î³
k
R
t+k+1
	â€‹

+Î³
n
V(S
t+n
	â€‹

)

and

ğ‘‰
(
ğ‘†
ğ‘¡
)
â†
ğ‘‰
(
ğ‘†
ğ‘¡
)
+
ğ›¼
(
ğº
ğ‘¡
(
ğ‘›
)
âˆ’
ğ‘‰
(
ğ‘†
ğ‘¡
)
)
V(S
t
	â€‹

)â†V(S
t
	â€‹

)+Î±(G
t
(n)
	â€‹

âˆ’V(S
t
	â€‹

))
Function Approximation

The value function is approximated linearly:

ğ‘‰
^
(
ğ‘ 
)
=
ğ‘¤
ğ‘‡
ğ‘¥
(
ğ‘ 
)
V
^
(s)=w
T
x(s)

where 
ğ‘¥
(
ğ‘ 
)
x(s) is the feature vector.
Weights are updated by:

ğ‘¤
â†
ğ‘¤
+
ğ›¼
(
ğº
ğ‘¡
(
ğ‘›
)
âˆ’
ğ‘‰
^
(
ğ‘†
ğ‘¡
)
)
ğ‘¥
(
ğ‘†
ğ‘¡
)
wâ†w+Î±(G
t
(n)
	â€‹

âˆ’
V
^
(S
t
	â€‹

))x(S
t
	â€‹

)
Feature Representations

Polynomial basis â€“ simple nonlinear expansion

Fourier basis â€“ smooth and flexible representation

State aggregation â€“ coarse grouping of states

Tile coding â€“ overlapping tilings for local generalization

Example Parameters
Parameter	Description	Example
num_states	Number of states	19
discount	Î³ (discount factor)	1.0
n_values	n-step values	[1, 2, 4, 8, 16]
alpha	Step size	0.01
basis	Feature type	"fourier"
order	Basis order	3
Files

random_walk_fa.py â€“ Core TD learning and environment logic

bootstrapping.ipynb â€“ n-step TD performance visualization

polynomials_vs_fourier.ipynb â€“ Comparison of basis functions

state_aggregation.ipynb â€“ Aggregation experiments

tile_coding.ipynb â€“ Tile coding demonstration

requirements.txt â€“ Dependencies

Results

Fourier basis converges faster and more smoothly

State aggregation improves stability but reduces precision

Higher n-step values balance bias and variance effectively

Summary

This project demonstrates how function approximation enables generalization in TD learning, bridging the gap between tabular and continuous-state reinforcement learning.
It highlights how feature selection and n-step updates shape learning performance and stability.