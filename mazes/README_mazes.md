Maze Navigation â€“ Dyna-Q, Dyna-Q+, and Prioritized Sweeping
Overview

This project explores model-based reinforcement learning through the Dyna architecture, applied to a maze navigation problem.
It compares three algorithms commonly used in planning and decision-making tasks:

Dyna-Q â€” classical model-based reinforcement learning

Dyna-Q+ â€” adds time-based exploration bonuses for better adaptability

Prioritized Sweeping â€” focuses planning on high-priority stateâ€“action updates

These methods are widely used in autonomous navigation, robot path planning, and simulation-based reinforcement learning research.

How It Works
Environment

Grid world: 6Ã—9

Start: (2, 0), Goal: (0, 8)

Obstacles block certain cells; the maze can change mid-training

Rewards: +1 for reaching the goal, 0 otherwise

Transitions are deterministic

Algorithms

Dyna-Q
Combines real experience and simulated model-based planning:

ğ‘„
(
ğ‘ 
,
ğ‘
)
â†
ğ‘„
(
ğ‘ 
,
ğ‘
)
+
ğ›¼
[
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
â€²
)
âˆ’
ğ‘„
(
ğ‘ 
,
ğ‘
)
]
Q(s,a)â†Q(s,a)+Î±[r+Î³
a
â€²
max
	â€‹

Q(s
â€²
,a
â€²
)âˆ’Q(s,a)]

Dyna-Q+
Adds a time-based exploration bonus:

ğ‘Ÿ
â€²
=
ğ‘Ÿ
+
ğœ…
â‹…
ğ‘¡
âˆ’
ğœ
(
ğ‘ 
,
ğ‘
)
r
â€²
=r+Îºâ‹…
tâˆ’Ï„(s,a)
	â€‹


Prioritized Sweeping
Uses a priority queue to update the most important transitions first.

Key Parameters
Parameter	Description	Example
discount (Î³)	Discount factor	0.95
step_size (Î±)	Learning rate	0.1
exploration_probability (Îµ)	Îµ-greedy policy	0.1
time_weight (Îº)	Dyna-Q+ exploration bonus	1e-4
planning_steps	Number of simulated updates	5
threshold (Î¸)	Priority threshold	0.01
runs	Averaged training runs	10
Files

maze.py â€“ Maze environment and state transitions

models.py â€“ Environment models for Dyna-Q, Dyna-Q+, and Prioritized Sweeping

functions.py â€“ Core algorithm logic

dyna.py â€“ Parameter configuration (DynaParams)

dyna_maze.ipynb â€“ Dyna-Q vs Dyna-Q+ training visualization

changing_maze.ipynb â€“ Adaptation to maze changes

prioritized_sweeping.ipynb â€“ Efficient planning demonstration

requirements.txt â€“ Dependencies

Results

Dyna-Q: Fast initial learning but struggles when the environment changes.

Dyna-Q+: Adapts faster to new obstacles using exploration bonuses.

Prioritized Sweeping: Achieves efficient learning with fewer model updates.

Summary

These experiments show how model-based planning, exploration bonuses, and priority-driven updates accelerate learning and adaptability in reinforcement learning systems.
They highlight the trade-off between exploration, efficiency, and computational cost in dynamic environments.