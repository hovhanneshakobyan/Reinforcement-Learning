Random Walk: n-step Temporal Difference Learning
Overview

This project implements the Random Walk environment to study n-step Temporal Difference (TD) learning, a core algorithm in reinforcement learning.
An agent starts from the center of a 19-state chain and learns to estimate state values based on returns over n future steps, analyzing the biasâ€“variance trade-off as n varies.

How It Works
Environment

States: 19 non-terminal states + 2 terminal states

Start: center of the chain

Transitions: move left or right with probability 0.5

Rewards:

âˆ’1 at the left terminal

+1 at the right terminal

0 otherwise

n-step TD Learning

For each state:

ğº
ğ‘¡
(
ğ‘›
)
=
âˆ‘
ğ‘˜
=
0
ğ‘›
âˆ’
1
ğ›¾
ğ‘˜
ğ‘…
ğ‘¡
+
ğ‘˜
+
1
+
ğ›¾
ğ‘›
ğ‘‰
(
ğ‘†
ğ‘¡
+
ğ‘›
)
G
t
(n)
	â€‹

=
k=0
âˆ‘
nâˆ’1
	â€‹

Î³
k
R
t+k+1
	â€‹

+Î³
n
V(S
t+n
	â€‹

)
ğ‘‰
(
ğ‘†
ğ‘¡
)
â†
ğ‘‰
(
ğ‘†
ğ‘¡
)
+
ğ›¼
â€‰
(
ğº
ğ‘¡
(
ğ‘›
)
âˆ’
ğ‘‰
(
ğ‘†
ğ‘¡
)
)
V(S
t
	â€‹

)â†V(S
t
	â€‹

)+Î±(G
t
(n)
	â€‹

âˆ’V(S
t
	â€‹

))

Smaller n â†’ more bias, less variance

Larger n â†’ less bias, more variance

Example Parameters
Parameter	Description	Example
states_number	Number of non-terminal states	19
discount	Discount factor (Î³)	1.0
steps	n-step values	1, 2, 4, 8, ...
step_sizes	Learning rates (Î±)	0.0â€“1.0
episodes	Episodes per run	10
runs	Independent runs	100
Files

random_walk.py â€“ Environment setup and n-step TD updates

random_walk.ipynb â€“ Simulations, RMSE evaluation, and plots

requirements.txt â€“ Dependencies

Results

Visualizes learning curves for different n values

Compares RMSE over episodes against the true state values

Demonstrates the biasâ€“variance trade-off in n-step TD learning

Summary

This project illustrates how n-step TD methods bridge the gap between Monte Carlo and TD(0) learning, offering insight into how multi-step updates influence learning speed and stability in reinforcement learning environments.